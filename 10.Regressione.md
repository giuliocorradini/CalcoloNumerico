# Regressione

Queste tecniche che andiamo a vedere si usano quando il rumore dei dati è casuale, rumore bianco, e non un errore
sistematico. So a priori di quale fenomeno si tratta, quindi scelgo di approssimare con una retta.

L'idea per risolvere questo problema è trovare una funzione $t = mx + q$ che minimizzi la distanza della retta tra
tutti i punti.

Non stiamo **interpolando**, ma stiamo costruendo una funzione che passi il più vicino possibile a tutti i dati, in modo
da avere un modello che alla fine matchi bene con quello che ho io.

Non conosco ancora i coefficienti della retta, ma so che posso esprimerla come $y = a_0 + a_1 x$.
La distanza tra un punto e questa retta si ricava come segue:

- il punto della retta di ascissa uguale al punto, ha ordinata: $r_i = a_0 + a_1 x_i$. La distanza tra l'ordinata del
punto e la retta è la differenza tra $y_i$ (del punto) e $a_0 + a_1 x_i$.

$$
l_i = | a_0 + a_1 x_i - y_i |
$$

Siccome voglio minimizzare la somma

$$
\sum _{i=1}^m (a_0 + a_1 x_i - y_i)^2 = Q(a_0, a_1)
$$

Adesso vogliamo trovare il minimo di questa funzione. Si può dimostrare che il minimo esiste ed è unico.
Possiamo anche generalizzare il criterio dei minimi quadrati a dimensioni superiori alla 2.

Noi lavoriamo con la **distanza cumulativa**, ma si potrebbe anche usare la distanza media o altri approci.

Se volessimo fare regressione con una parabola, andremmo a cercare la funzione $f(a_0, a_1, a_2; x) = a_0 + a_1 x + a_2
x^2$ che minimizza $Q(a_0, a_1, a_2)$.

$$
Q(a_0, a_1, a_2) = \sum_{i=1}^m (a_0 + a_1 x_i + a_2 x_i^2 - y_i)^2
$$

### Vettore q

$$
m = 4 \\
Q(a_0, a_1) = (a_0 + a_1 x_1 - y_1)^2 + (a_0 + a_1 x_2 - y_2)^2 + (a_0 + a_1 x_3 - y_3)^2 + (a_0 + a_1 x_4 - y_4)^2
$$

Il vettore q contiene tutti i $q_i = a_0 + a_1 x_i$

$$
q = \begin{pmatrix} a_0+a_1x_{1} \\ a_0+a_1x_{2} \\ ... \\ a_0+a_1x_{m}  \end{pmatrix}
$$

Posso quindi riscrivere la funzione Q come:

$$
Q(a_0, a_1) = ||q(a_0, a_1) - y||^2 = ...
$$

(visto che il quadrato della norma 2 è la somma dei quadrati del vettore).

allora definisco u come

$$
u = \begin{pmatrix} q_{1} - y_1 \\ q_{2} - y_2 \\ ... \\ q_{m} - y_m \end{pmatrix}
= q - y
$$

e ne deriva che

$$
... = {||u||_2}^2 = {||q - y||_2}^2
$$

Il vettore q si può anche ottenere come prodotto matrice vettore

$$
q = \begin{pmatrix} a_0+a_1x_{1} \\ a_0+a_1x_{2} \\ ... \\ a_0+a_1x_{m}  \end{pmatrix}
= \begin{pmatrix} 1 x_1 \\ 1 x_2 \\ ... \\ 1 x_m  \end{pmatrix} \begin{pmatrix} a_0 \\ a_1 \end{pmatrix}
= A \alpha
$$

Il caso $n=m$ corrisponde al polinomio di interpolazione.

## Minimizzazione

Nel caso non degenere tutte le colonne della matrice A sono linearmente indipendenti, mentre nel caso generale potrei
avere delle colonne linearmente dipendenti.

> Se la matrice $A \in \mathbb{R}^{m x n}$ ha rango n allora la soluzione del problema di minimo è unica

La matrice A ha rango n perché è una collezione di vettori in $\mathbb{R}^2$.

### Proprietà 1

HP: Se Q è una matrice ortogonale ($Q^T = Q^{-1}$) allora

TH: $||Qx||^2 = ||u||^2 = u^T u = Qx Qx = x^T Q^T Qx = x^T I x = x^T x$

### Proprietà 2

HP: $z \in \mathbb{R}^m$, $z = \begin{pmatrix} z_1 \\ z_2 \end{pmatrix}$ con $z_i \in \mathbb{R}^n$

TH: ||z||^2 = ||z_1||^2 + ||z_2||^2

riordino il vettore z in due vettori, il primo avrà dimensione n e il secondo m-n. Allora scrivo la norma di z per
esteso, poi raggruppo i termini dei sottovettori ed è dimostrata.

## Proprietà 3

Discende dalla fattorizzazione QR. Possiamo decomporre A (m x n) in Q ed R.

$$
A = Q \begin{pmatrix}R \\ 0 \end{pmatrix} \Leftrightarrow Q^T A = \begin{pmatrix}R \\ 0 \end{pmatrix}
$$

Se le righe non sono linearmente indipendenti bisogna usare un altro metodo come l'SVD.

$$
||A \alpha - y||^2 (P1)= || Q^T (A \alpha - y)||^2 = || Q^T A \alpha - Q^T y)||^2 =
|| \begin{pmatrix}R \\ 0 \end{pmatrix} \alpha - \overline{y}||^2
$$

se Q è una matrice ortogonale ($Q^T = Q^{-1}$) allora anche la sua trasposta è ortogonale.

$\begin{pmatrix}R \\ 0 \end{pmatrix} \alpha$ dà come risultato una matrice m x 1.

$\overline{y}_1$ indica il vettore delle prime n componenti di $\overline{y}$.

$$
||\begin{pmatrix}R \\ 0 \end{pmatrix} \alpha - \begin{pmatrix} \overline{y}_1 \\ \overline{y}_2 \end{pmatrix}||^2
= ||R \alpha - \overline{y}_1||^2 + ||\overline{y}_2||^2
$$

il minimo si ha per $||R \alpha - \overline{y}_1 || = 0$. La norma è nulla quando il vettore è nullo, quindi
$R \alpha - \overline{y}_1 = 0$, cioè $R \alpha = \overline{y}_1$.
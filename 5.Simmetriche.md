# Matrici simmetriche

Una matrice è simmetrica se $A = A^T$.

Possiamo sfruttare questa proprietà per fare una fattorizzazione più veloce con Gauss.

## Teorema di fattorizzazione di Gauss per matrici simmetriche

Se A è simmetrica e i minori principali sono diversi da 0 (come l'hp del teorema di fattorizzazione
per matrici generiche), allora esistono una matrice L triangolare inferiore con diagonale unitaria
e una matrice diagonale D con elementi diagonali diversi da zero tali che:

$A = LDL^T$

Quindi:

- L matrice triangolare con diagonale principale che ha tutti 1 (diagonale unitaria)

- D matrice diagonale (tutti 0 tranne sulla diagonale principale)

### Dimostrazione

Dal teorema di fattorizzazione di Gauss sappiamo che A si può fattorizzare come $A = LU$,
inoltre la U ha sulla diagonale principale dei valori che moltiplicati danno il valore di $det(A)$.

$$
D = \begin{pmatrix} u_{11} & \ & \ & 0 \\ \ & u_{22} & \  & \  \\ \ & \  & \ddots  & \  \\ 0 & \  & \  & u_{44}  \\  \end{pmatrix}
$$

Adesso nella relazione $A = LU$ faccio comparire $D$ con l'identità, quindi moltiplico per $DD^{-1}$.

Chiamando $R=D^{-1} U$, è triangolare superiore e ha diagonale unitaria.

Le matrici triangolari formano dei sottogruppi (moltiplicando due matrici diagonali superiori/inferiori
riottengo una matrice diagonale superiore/inferiore).

$D^{-1}$ contiene semplicemente i reciproci degli elementi di D sulla diagonale.

Moltiplicando $D^{-1} U$ succede che avremo una matrice triangolare superiore con diagonale unitaria,
e nelle righe avrò gli elementi precedenti divisi per quello che è nella diagonale.
Cioè $u_{ij} / u_{ii}$ solo per gli $i < j$.

Quindi adesso abbiamo ottenuto che $A = LDR$, ma sappiamo che $A=A^T$ quindi $A^T=(LDR)^T = R^T D L^T$;
sparisce il trasposto su D perché è diagonale, quindi simmetrica.

Possiamo dire che $LDR = R^T D L^T$, moltiplico da entrambi i lati per l'inversa: $LDR(L^T)^{-1} = R^T D L^T (L^T)^{-1}$
così da ottenere l'identità. Poi ho $L^{-1}LDR(L^T)^{-1} = L^{-1} R^T D$.

Alla fine abbiamo: $DR(L^T)^{-1} = L^{-1} R^T D$. Cioè un'uguaglianza tra una matrice triangolare superiore e una inferiore.
Ma allora $DR(L^T)^{-1}$ è diagonale, perciò $R(L^T)^{-1}$ è diagonale.

Sappiamo che l'ultima matrice ha diagonale unitaria dunque $R(L^T)^{-1} = I$. A questo punto $R = L^T$.

## Implementazione

Implicazione: devo calcolare meno elementi. Non uso più l'eliminazione di Gauss.

Si usa il metodo della pavimentazione. Sappiamo già come si decompone la matrice A (in $LDL^T$).

Al primo passo j=1 (colonna) e i=j..n=1 (prima riga):

i = 1 quindi $d_{11} = a_{11}$, mentre per gli i>1 $l_{i1} = a_{i1}/d_{11}$.

Considero la prima colonna, divido tutto per il pivot. Gli elementi della sottocolonna vengono
ricopiati anche nella riga, tanto la fattorizzazione produce $L$ e $L^T$.

Al secondo passo j=2, i=2:

i = 2 abbiamo $a_{22} = d_{22} + d_{11} {l_{21}} ^2 \rightarrow d_{22} = a_{22} - d_{11} {l_{21}} ^2$.

In generale nota la riga i-esima, possiamo trovare la (i+1)-esima.

```pseudocode
for j = 1, ..., n
    d_jj = a_jj - sum(k=1..j-1) l_jk^2 * d_kk
    for i = j + 1, ..., n
        l_ij = (a_ij - sum(k=1..j-1) l_ik * d_kk * l_jk) / d_jj
```

## Complessità

Al passo j effettuo $2(j-1) + (j-1)(n-j)$ prodotti. Questo costo viene affrontanto n
volte perché itero sulle righe. Il costo totale è di O(n^3 / 6), rispetto a Gauss LU risparmiamo
un fattore 2.

Testiamo il metodo in matlab.

## Usi

Adesso sappiamo che possiamo sostituire A con $LDL^T$, per risolvere il sistema posso fare:

$LDL^T x = b \rightarrow Lz = b \rightarrow Dy=z \rightarrow L^T x = y$.

Chiamiamo $DL^T x = z$ e $L^T x = y$.

$Lz$ è un sistema triangolare inferiore: sostituzione in avanti.


$Dy$ è un sistema diagonale, si risolve con n divisioni.

$L^T x$ è un sistema triangolare superiore: sostituzione all'indietro.

Il costo dell'algoritmo di risoluzione è tutto nella risoluzione.

# Definite positive

Una matrice simmetrica definita positiva è simmetrica ($a_{ij} = a_{ji}$) con una proprietà aggiuntiva:
$x^T Ax \ge 0 \ \forall x \in R^n$ e $x^T Ax = 0 \Leftrightarrow x=0$.

Tutti i minori principali sono positivi e gli autovalori sono reali positivi.

A livello dimensionale $x^T Ax$ è un prodotto di una riga ($x^T$), mentre $x$ è una colonna. Da $Ax$ viene fuori una colonna,
che moltiplicata per $x^T$ mi restituisce uno scalare.

Il prodotto $x^T\ x$ mi dà la norma del vettore $x$: la somma dei quadrati delle componenti.

La matrice I é _definita positiva_.

## Algoritmo ad-hoc

Posso ottenere un algoritmo più stabile con una fattorizzazione ad-hoc, anche se il tempo non migliora e rimaniamo
comunque su $O(n^3/6)$.

## Teorema di fattorizzazione di Cholesky

> Una matrice simmetrica $A \in R^{n\ x\ n}$ è definita positiva se e solo se esiste una matrice triangolare
inferiore L con elementi diagonali positivi tale che $A = LL^T$.

Questo è un $\Leftrightarrow$ quindi se scopro che la matrice si fattorizza come $LL^T$ so che è simmetrica
definita positiva. Matlab parte sempre usando Cholesky per fattorizzare.

## Dimostrazione

Voglio dimostrare:

> Una matrice $A$ è simmetrica definita positiva $\Leftrightarrow \exists$ L triangolare inferiore tale che $LL^T=A$

### (1) Verso $\to$

Per ipotesi:

> $A_{nxn}$ è simmetrica e definita positiva.

Tesi:

> $\exists L_{nxn}$ tiangolare inferiore, i cui elementi della diagonale sono $\gt 0$. A si fattorizza come $LL^T$.

Siccome A è simmetrica, si fattorizza come $LDL^T$ dal teorema di fattorizzazione di Gauss per matrici simmetriche.

Inoltre A è definita positiva quindi: $x^tAx \gt 0$ (tolgo l'uguale per la seconda condizione delle matrici definite positive),
lo riscriviamo come $x^T L D L^T x > 0$. Chiamo $y = L^T x$, e dal teorema di Binet $y \ne 0$.

$y^T = (L^Tx)^T = x^T L$. Quindi $x^T L D L^T x = y^T D y \gt 0 \forall y \in R^n \setminus \{0\}$.

Allora la matrice D è definita positiva ed è diagonale (dal teorema di fattorizzazione LDL).
Tutti i minori principali sono positivi. Queste due cose implicano che gli elementi di D siano tutti $\gt 0$.

Mi conviene definire $\mathcal{L} = L \Delta$, dove la matrice $\Delta$ ha le radici quadrate degli elementi di D
sulla diagonale.
Moltiplicando $\Delta \Delta$ otteniamo di nuovo D.

### (2) Verso $\gets$

Per ipotesi:

> $\exists\ \mathcal{L}_{nxn}$ tiangolare inferiore, i cui elementi della diagonale sono $\gt 0$. A si fattorizza come $LL^T$.

Tesi:

> $A_{nxn}$ è simmetrica e definita positiva.

Sia x un vettore in $\mathbb{R}^n$, allora abbiamo che $x^t A x = x^t \mathcal{L} \mathcal{L}^T x = y^Ty = ||y||_2$.

Recuperiamo la disuguaglianza che adesso scriviamo come $||y||_2 \gt 0$. Poi se $y_i = 0 \forall i$
allora $x^TAx = ||y||_2 = 0$.

## Codice

Posso riusare il codice che ho fatto per la fattorizzazione LD di Gauss.
La matrice $\mathcal{L} = L \Delta$, quindi:

$$\mathcal{l}_{jj} = \sqrt{d_{jj}}$$

$$\mathcal{l}_{jk} = l_{jk}\sqrt{d_{kk}} \ \forall k=1,...,j-1$$

$$
\begin{pmatrix} l_{11} & 0 & \dots & 0 \\ l_{21} & l_{22} & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots & \\l_{n1} & l_{n2} & \dots & l_{nn} \end{pmatrix}

\begin{pmatrix} \sqrt{d}_{11} & 0 & \dots & 0 \\ 0 & \sqrt{d}_{22} & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots & \\0 & 0 & \dots & \sqrt{d}_{nn} \end{pmatrix}

=

\begin{pmatrix} \sqrt{d}_{11} & 0 & \dots & 0 \\ l_{21}\sqrt{d}_{11} & \sqrt{d}_{22} & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots & \\ l_{n1}\sqrt{d}_{11} & l_{n2}\sqrt{d}_{22} & \dots & \sqrt{d}_{nn} \end{pmatrix}
$$

L'algoritmo presenta stabilità forte: gli elementi della fattorizzazione di Cholesky sono maggiorati da delle costanti
che non dipendono dalla dimensione della matrice.

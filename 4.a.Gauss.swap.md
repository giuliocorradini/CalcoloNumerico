# Gauss con scambio di righe

## Classi di matrici che soddisfano le ipotesi

Esiste una classe di matrici che soddisfa le ipotesi del teorema di fattorizzazione di Gauss, per cui i minori principali
siano diversi da 0 e quindi i pivot siano non nulli: le matrici strettamente a diagonale dominante per righe.

Una matrice quadrata A si dice strettamente a diagonale dominante per righe se

$$
\sum_{j=1, j\ne i}^{n} |a_ij| \lt |a_ii| \\
\forall i = 1, ..., n
$$

> ovvero la somma di ogni riga in valore assoluto, escluso l'elemento diagonale, è minore del valore assoluto
> dell'elemento diagonale.

Non tutte le matrici invertibili soddisfano l'ipotesi che tutti i minori principali siano diversi da 0.

## Strategia di pivoting

Rendiamo la fattorizzazione di Gauss applicabile a tutte le matrici invertibili.

L'idea è quella di scambiare, durante il processo di fattorizzazone, due righe della matrice A.
L'importante è che se scelgo di scambiare la riga k con la riga r, $r \ge k$ in modo da non distruggere
la struttura triangolare della matrice fin'ora creata.
Scambiare due righe equivale a scambiare due equazioni nel sistema $Ax = b$.

### Matrici di permutazioni elementari

Sono matrici che esprimono lo scambio di due righe o due colonne, si costruiscono a partire dalla matrice identità
e si scambiano la riga (o la colonna) i-esima con la colonna j-esima.

Possiamo anche leggerle come matrici in cui se c'è 1 in una posizione, allora la riga i-esima finirà nella riga j-esima.

Se moltiplico una matrice di permutazione elementare a sinistra sto scambiando le righe, se la moltiplico a destra
sto scambiando le colonne: $P_{ij}A$ oppure $AP_{ij}$.

Le matrici di permutazione elementari sono:

- nonsingolari, e il determinante è uguale a -1;

- simmetrice: $P_{ij} = P_{ij}^T$;

- ortogonali: $P_{ij} = P_{ij}^{-1}$.

Un prodotto di matrici di permutazione elementare viene detto matrice di permutazione, che è ortogonale perché
è un prodotto di matrici ortogonali.

## Nuovo algoritmo

Partiamo sempre dall'ipotesi che A sia nonsingolare, quindi i minori principali sono diversi da 0 ovvero esiste
un elemento della prima colonna che sia diverso da 0.

Lo portiamo nella prima riga con una permutazione $P_1$ che scambia la riga 1 con la riga r.

Se l'elemento $a_{11} \ne 0$ posso anche scegliere $P_1=I$.

A questo punto procedo col metodo di Gauss e calcolo la trasformazione elementare di Gauss $L_1$ per la matrice $P_1A$.

Consideriamo quindi la sottomatrice $\tilde A_2$ ottenuta eliminando la prima riga e colonna da $A_2$ e osserviamo
che la matrice è nonsingolare perché prodotto di matrici nonsingolari (in particolare il determinante
di $A_2$ è uguale a $-det(A)$ se la permutazione elementare è diversa da $I$).

Dal teorema di Laplace $det(A_2) = a_{11}^{(2)} det(\tilde A_2)$, in particolare è diverso da 0 perché
$\tilde A_2$ non singolare
implica che il determinante sia diverso da 0.

Siccome $\tilde A_2$ è non singolare, segue lo stesso ragionamento di prima sulla colonna 2 e si procede nuovamente
con un'eventuale scambio di righe.

### Generalizzazione

$$
A_k = L_{k-1}P_{k-1} ... L_2 P_2 L_1 P_1 A
$$

Al passo k consideriamo la sottomatrice $\tilde A_k$ che è nonsingolare (perché A lo è).

Il determinante della matrice $det(A_k) = a_11 a_22 ... a_{k-1,k-1} det(\tilde A_k)$.

Esiste almeno un elemento diverso da 0 nella colonna k-esima della matrice $\tilde A_k$, visto che è non singolare.

### Finale

Se A è una matrice quadrata nonsingolare, allora esiste una matrice di permutazione P, una matrice triangolare superiore
U e una matrice triangolare inferiore L tali che $PA = LU$.

La matrice U è definita per costruzione, inoltre P è il prodotto delle permutazioni semplici in ordine $P_{n-1} ... P_1$.

Infine:

$$
L = \prod_{k=1}^{n-1} P_{n-1} ... P_{k+1} L_k^{-1}
$$

ovvero ogni sottocolonna contiene i moltiplicatori permutati rispetto alle permutazioni sulle righe dei passi successivi.

Il sistema lineare si risolve come prima, avendo l'accortezza di permutare anche b secondo P.

$$
Ax = b \\
PAx = Pb \\
LUx = Pb
$$

$$
\begin{cases}
    Ly = Pb \\
    Ux = y
\end{cases}
$$

La soluzione adesso non è ovviamente unica perché potrei scegliere righe diverse per la permutazione.
Scegliere un pivot piuttosto che un altro influenza gli elementi di L e di conseguenza può far diventare instabili
gli algoritmi di risoluzione per sostituzione; se gli elementi delle sottocolonne sono troppo grandi
rispetto alla diagonale principale.

Quindi l'algoritmo di fattorizzazione di Gauss è instabile, ma possiamo sfruttare la libertà di scelta nel pivot per
renderlo stabile.

## Pivoting parziale

Ad ogni passo k, si sceglie come elemento pivot il massimo elemento in valore assoluto della prima colonna della
sottomatrice $\tilde A_k$.

Conseguenza del pivoting parziale è che il massimo finisce al denominatore di ogni moltiplicatore, quindi in valore assoluto
è strettamente minore di 1.

$$
|m_{ik}| \lt 1
$$

tuttavia questo introduce un costo ulteriore, ovvero a ogni passo devo fare $n-k+1 = O( \frac{n^2}{2} )$ confronti.

L'algoritmo risulta quindi più stabile.

Se la matrice A è strettamente diagonale per righe o per colonne, la condizione di pivoting parziale è automaticamente
soddisfatta e si possono evitare confronti, nonché scambi di righe e colonne. Infatti in una matrice strettamente diagonale
per colonne, gli elementi della diagonale maggiorano in valore assoluto la somma degli elementi della colonna e in particolare
maggiorano ogni elemento della colonna.

## Pivoting totale

Posso individuare anche l'elemento più grande nella sottomatrice $\tilde A_k$ tale che $|a_{rs}^{(k)}| = \max_{i,j=k,...,n}
|a_{ij}^{(k)}|$.

La permutazione associata è $P_{kr}$ per le righe e $Q_{ks}$ per le colonne. Avrò quindi un sistema $PAQ = LU$.

Siccome al passo k devo confrontare gli elementi della sottomatrice, il costo dell'operazione di pivoting parziale segue
$O(\frac{n^3}{3})$, che è simile a quello della fattorizzazione.

Nella pratica si usa il pivoting parziale per il trade-off tra stabilità e prestazioni.

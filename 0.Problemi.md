# Introduzione

Usare il calcolatore ci costringe a usare l'aritmetica finita, che rispetto a quella esatta introduce
degli errori.

Si fanno sequenze di operazioni e ognuna introduce potenzialmente un errore, ed elabora anche l'errore
della computazione precedente. Quindi l'errore si propaga e dipende sia dal modo in cui eseguo le operazioni
che dalle operazioni che eseguo.

Devo sapere quanto è sensibile il problema che sto affrontando alla precisione dei dati.

Dobbiamo fare analisi del **condizionamento del problema** che stiamo affrontando. Dopo il problema va affrontato
e qui interviene la progettazione di algoritmi numerici cercando (se possibile) di limitare la propagazione
degli errori che inevitabilmente vengono commessi a ogni operazione elementare.

Questo va sotto il nome di **stabilità dell'algoritmo** e riguarda il modo in cui risolviamo il problema.
È un criterio parallelo a quello della rapidità dell'algoritmo con cui risolviamo il problema.

## Esempio

Somma di tre numeri, ho tre algoritmi diversi per risolvere il problema:

$\alpha = x + y + z$

Devo assumere che non sto partendo da x, y e z ma dalle loro _approssimazioni floating point_.

La soluzione del calcolatore sarà: $fl(fl(fl(x)+fl(y))+fl(z))$.

Quindi in termini matematici dico che per un addendo $s$, $fl(s) = s(1 + \epsilon)$. Versione perturbata
del numero reale.

Il primo passo della somma è:

$fl(fl(x) + fl(y)) =$

$fl(x(1+\epsilon _x) + y(1+\epsilon _y)) =$

$(x(1+\epsilon _x) + y(1+\epsilon _y))(1+\epsilon _1)$

Adesso voglio calcolare l'errore relativo di questa somma, ovvero sapere quanto dista da $\alpha$.

Il risultato finale è $\alpha ^* = ((x(1+\epsilon_x)+y(1+\epsilon_y))(1+\epsilon_1)+z(1+\epsilon_z))(1+\epsilon_2)$, che posso espandere in:

$x+x\epsilon_x+y+y\epsilon_y+x\epsilon_1+x\epsilon_1\epsilon_x+y\epsilon_1+y\epsilon_y\epsilon_1+z+z\epsilon_z+x\epsilon_2+x\epsilon_x\epsilon_2+y\epsilon_2+y\epsilon_y\epsilon_2+x\epsilon_1\epsilon_2$

L'errore relativo $\frac{\alpha^* - \alpha}{\alpha}$, il numeratore mi dice di togliere x+y+z da $\alpha^*$.

Siamo partiti dall'assunzione che ogni $\epsilon$ sia minore o uguale della precisione di macchina, quindi in prima approssimazione posso tenere solo i termini con un solo epsilon, perché quelli con gli errori quadratici sono infinitesimi molto più piccoli.

$Er = \frac{\alpha^* - \alpha}{\alpha} ~= \frac{x}{x+y+z}\epsilon_x + \frac{y}{x+y+z} \epsilon_y + \frac{z}{x+y+z} \epsilon_z + \frac{x+y}{x+y+z} \epsilon_1 + \epsilon_2$

Gli addendi che dipendono solo da x, y, e z si chiamano errore inerente $E_{in}$, mentre gli altri due addendi compongono l'errore $E_{alg}$ algoritmico.
Un algoritmo che minimizza $E_{alg}$ si dice stabile. Da questo vediamo che ci conviene sommare prima numeri piccoli e poi numeri grandi alla fine.

L'errore inerente è quello commesso se le operazioni fossero eseguite in aritmetica esatta (ovvero i due $\epsilon_n = 0$).

## Condizionamento

Un problema è ben condizionato se a piccole perturbazioni sui dati, corrispondono altrettanto piccole variazioni delle soluzioni.
Dò per scontato che parto da dati perturbati, ma se la mia precisione è nell'ordine della precisione di macchina e il problema è
ben condizionato, allora mi aspetto piccole variazioni dovute alla propagazione dell'errore.

Queste considerazioni sono da leggersi al netto dell'aritmetica esatta.

Un problema mal condizionato mi fa variare sensibilmente (di ordini di grandezza) di una soluzione a piccole variazioni dell'input.

Facciamo finta di voler risolvere l'equazione $x^2 - 4x + c$, le soluzioni sono $x_{1,2} = 2 \pm \sqrt{4-c}$.

Se prendo un $c=4 \Rightarrow \alpha=2$. Se prendo un c perturbato $c=4-10^{-6} \Rightarrow \alpha=2 - 10^{-3}$.

Questo non è un problema ben condizionato perché la perturbazione sul dato ha generato una variazione di $\frac{1}{2} * 10^{-3}$. Qui non c'è errore algoritmico ma solo errore inerente.

La stabilità non viene solitamente valutata in assoluto, ma relativamente a un altro algoritmo (anche se ha senso anche in assoluto).

L'altro criterio per valutare gli algoritmi numerici è la complessità computazionale, ovvero il numero di operazioni aritmetiche necessarie per portare a termine
l'algoritmo.

La stabilità invece è la capacità dell'algoritmo di non amplificare troppo gli errori.

## Counterexamples

Meno operazioni migliorano sempre la stabilità? No -> esempio del punto medio.

Formula 1: $\frac{a+b}{2}$

Formula 2: $a + \frac{b-a}{2}$

Nel caso di un sistema decimale con 3 cifre significative, il risultato della formula 1 con a=0.983 e b=0.986 ricade fuori dall'intervallo di a e b quindi
non va bene.

La formula 2 ha una somma in più ma è più stabile.

# Metodi iterativi

Sono metodi che risolvono iterativamente un sistema di equazioni, cioè applicando
n-volte lo stesso blocco di codice per arrivare a costruire una soluzione.

Il sistema da risolvere:

$$Ax = b$$

a ogni passo genero un vettore $x^{(k)}$ che per $k \to \infty$ converge alla soluzione $x^*$.

Per verificare che il vettore k-esimo sia soluzione si confronta il prodotto $Ax^{(k)}$ con b.

## Calcoli esatti

I metodi iterativi approssimano le soluzioni, ed è accettabile siccome lavoriamo su dei calcolatori
che manipolano dei float la cui aritmetica è finita.

L'idea della soluzione è:

![](passi_della_soluzione.png)

Ho un punto che si avvicina alla soluzione esatta sempre di più a ogni passo dell'algoritmo.
Si può rendere con una successione.

# Successioni

$\{x_n\}_{n \in \mathbb{N}}$ è una successione di numeri reali. Possiamo andare a vedere cosa succede
al limite della successione.

## Definizione

La successione di vettori è $\{x^{(k)}\}_{k \in \mathbb{N}}$, dove $x^{(k)} \in \mathbb{R}^n\  \forall k$.

Le successioni sono completamente libere, ma devono convergere alla soluzione del nostro sistema $x^*$.
Cioè voglio che $x^{(k)} \to x^*$ per $k \to \infty$. Se voglio scriverlo come limite posso sfruttare
la norma vettoriale della differenza $x^{(k)} - x^*$; una qualche norma $||•||$.

Voglio l'informazione sulla distanza da un punto come un valore scalare, per questo introduco la norma vettoriale.

Posso lavorare anche con un vettore di distanze. Si può dimostrare che se convergono le norme, allora convergono
anche le componenti del vettore e viceversa.

# Metodi generali

Possiamo riformulare il problema originale per generare dei metodi generali di risoluzione.

Data una matrice quadrata $M$ nonsingolare, il sistema si può riscrivere come:

$$
Mx = Mx + b - Ax
$$

$$Mx = (M - A)x + b$$

$$x = M^{-1}(M - A)x +  M^{-1} b$$

$$x = (I - M^{-1}A)x + M^{-1}b$$

Mi serve l'ipotesi di M nonsingolare cosicché esista l'inversa di M.

Chiamiamo $G := (I - M^{-1}A)$ e $c := M^{-1}b$. A questo punto il sistema si riformula come $x = Gx + c$.

Adesso l'incognita compare due volte, perché ci serve? Perché posso aggiornare la variabile, quindi lo userò
come $x^{(k+1)} = Gx^{(k)} + c$. La riformulazione adesso viene detta *relazione di ricorrenza*.

- $G := (I - M^{-1}A)$ è la matrice di iterazioni

- $M$ è la matrice del metodo, l'unica cosa che abbiamo introdotto e che possiamo scegliere, oltre al punto iniziale.
Fa la differenza. La scelta di M fa sì che il metodo converga (o no), velocemente e lentamente alla soluzione.

Un metodo iterativo è convergente se per ogni $x^{(0)}$ viene generata una soluzione del sistema.

# Criterio di arresto

Bisogna definire un certo $\epsilon$ entro cui fermarsi, per far terminare l'algoritmo entro un numero finito di passi
garantendo che la soluzione sia entro un certo numero di passi.

## Stima errore

Il criterio è relativo, calcolato sul quoziente tra una soluzione alla k-esima iterazione a la successiva.
In questo modo si generalizza il concetto di errore che si può applicare a tutti i sistemi, senza aggiustare
la tolleranza e l'ordine di grandezza tutte le volte.

L'altro modo per stimare l'errore è il calcolo del residuo. Il residuo $r^{(k)} := b - Ax^{(k)}$, è nullo nella
soluzione.

L'ultimo criterio è il numero di iterazioni, per fermare in modo sicuro l'algoritmo. Viene detto **parametro di
salvaguardia**.

## Teorema di convergenza

> Se ||G|| < 1 allora il metodo iterativo $x^{(k+1)} = G x^{(k)} + c$ è convergente.

$$se \ ||G|| \lt 1 \Rightarrow \lim_{k \to \infty} x^{(k)} = x^*$$

La norma di una matrice gode della proprietà submoltiplicativa, cioè $$||AB||_• \le ||A||_•||B||_•$$

Quando moltiplico un vettore per una matrice vuol dire che sto "scalando della norma" quel vettore. Quindi se la norma
della mia matrice è minore di 1 allora sicuramente il vettore non si allontana più della sua dimensione.

### Dimostrazione

Definiamo il vettore $e^{(k)} = x^{(k)} - x^*$, cioè l'errore al passo k. Dire che ho convergenza significa affermare
che $\lim_{k \to \infty} e^{(k)} = 0$.

$$
e^{(k)} = x^{(k)} - x^*
$$

adesso sostituisco $x^(k)$ con la sua definizione e $x^*$ esplicitandolo come soluzione di $x = Gx + c$

$$
= Gx^{(k-1)} + c - Gx^* - c*
$$

$$
= Gx^{(k-1)} - Gx^* = G(x^{(k-1)} - x^*)
$$

$$
= G e^{(k-1)}
$$

ma anche $e^{(k-1)} = G e^{(k-2)}$.

Quindi $e^{(k)} = G^2 e^{(k-2)}$ cioè dopo k passi arrivo a $e^{(k)} = G^k e^{(0)}$.

Praticamente ho fattorizzato $x^{(k)} - x^*$, allora posso dire che

$$
0 \le \lim_{k \to \infty} ||x^{(k)} - x^*|| \le \lim_{k \to \infty} ||G^k||\ ||e^{(0)}||
$$

$$
\le \lim_{k \to \infty} ||G||^k\ ||e^{(0)}||
$$

$$
\le (\lim_{k \to \infty} ||G||^k)\ ||e^{(0)}||
= ||e^{(0)}|| (\lim_{k \to \infty} ||G||^k)
$$

perché solo $||G||^k$ dipende dal limite. Adesso posso applicare l'ipotesi, siccome il termine nel limite è minore di 1,
moltiplicando k volte faccio diventare il risultato sempre più piccolo fino a 0.

Formalizziamo:

$$
||G|| = \frac{1}{p}
\Rightarrow
||G||^k = \frac{1}{p^k} \to 0
$$

## Velocità di convergenza

Dipende dal raggio spettrale della matrice G. Raggio spettrale piccolo corrisponde ad algoritmo veloce.

$$\rho (G) < 1 \Leftrightarrow \lim_{k \to +\infty} x_k = x^*$$

## Stima errore

Ho ottenuto che $x^{(k+1)} - x^{(k)} = (G-I)(x^{(k)} - x^*)$

Possiamo anche dimostare che (G-I) è nonsingolare.

$$G-I = I - M^{-1}A - I = - M^{-1}A$$

siccome ho scelto sia M che A nonsingolare, dal teorema di Binet anche $det(G-I) \ne 0$.

Quindi chiamo $\epsilon = \tau ||(G-I)^{-1}||$. Tanto più G è simile a I, tanto più tau approssima bene epsilon.

$$
||x_k - x^*|| \lt \epsilon
$$

fissato $\tau \gt 0$, $||x^{(k+1)} - x^{(k)}|| \lt \tau$ allora

$$
||x_k - x^*|| \le \tau ||(G-I)^{-1}||
$$

che si ricava scrivendo $x^{(k+1)} - x^{(k)}$ in funzione di $x^{(k)}$ e $x^*$. Poi ottengo il termine con $x^*$ in
funzione di quello con $x^{(k+1)}$ e si passa all'equazione delle norme; si applica la disuguaglianza di Cauchy-Schwarz.

$$
||x^{(k)} - x^*|| = ||(G-I)^{-1} x^{(k+1)} - x^{(k)}|| \\
||x^{(k)} - x^*|| \le ||(G-I)^{-1}|| \ ||x^{(k+1)} - x^{(k)}|| \\
$$

divido tutto per $||x^{(k+1)}||$ per ottenere l'errore relativo di $\frac{||x^{(k)} - x^*||}{||x^{(k+1)}||}$ che
approssima l'errore di relativo di $\frac{||x^{(k)} - x^*||}{||x^*||}$ se assumiamo che $x^{(k+1)}$ e $x^*$ abbiano
lo stesso ordine di grandezza.

Possiamo quindi dire che:

$$
\frac{||x^{(k)} - x^*||}{||x^*||} %errore relativo sulla soluzione

\simeq \frac{||x^{(k)} - x^*||}{||x^{(k+1)}||} \le \frac{||x^{(k+1)} - x^{(k)}||}{||x^{(k+1)}||}\ ||(G-I)^{-1}||
$$

di solito il coefficiente $||(G-I)^{-1}||$ non è noto, e se non è troppo grande e il termine $\frac{||x^{(k+1)} - x^{(k)}||}{||x^{(k+1)}||}$
è minore di $\tau$ allora

$$
\frac{||x^{(k)} - x^*||}{||x^*||} \simeq \tau
$$

## Metodo del residuo

Valuto $r(\tilde x) = A\tilde x - b$, che è il residuo del numero. Più la norma è vicina a 0, più la soluzione è corretta.
Il residuo è nullo in corrispondenza della soluzione.

### Proprietà

Sia fissato $\tau \gt 0$, se $\frac{||r^{(k)}||}{||b||} \le \tau$ allora $\frac{||x^{(k)} - x^*||}{||x^*||}$ errore
relativo sulla soluzione, è limitato da $k(A) \tau$.

$k(A)$ è il numero di condizionamento della matrice A. Il malcondizionamento porta a problematiche sulla soluzione.

Dimostrazione:
 
$$
\frac{||x^{(k)} - x^*||}{||x^*||} \le k(A) \frac{||r^{(k)}||}{||b||}
\Rightarrow
\frac{||x^{(k)} - x^*||}{||x^*||} \le k(A) \tau
$$

la prima riga è la definizione del numero di condizionamento. Slide 168.

Abbiamo quindi definito due criteri per la stima dell'errore.
Mi fermo quando ho soddisfato uno dei criteri di arresto. Quando valgono, allora
$x^{(k)}-x^* \le \epsilon ||x^{(k+1)}||$.

Scritto così so quantificare $\epsilon$.

Sono utili i due criteri perché se uno porta una convergenza molto lenta, l'altro potrebbe essere più veloce.

Si può introdurre un parametro di salvaguardia, per bloccare forzatamente l'esecuzione visto che il rischio di questi
algoritmi è andare avanti all'infinito.

Fisso un certo $k_{max}$ tale che $k \le k_{max}$.

## Vantaggi e costi

I metodi iterativi diventano competitivi quando faccio n operazioni, altrimenti se faccio $n^2$ iterazioni non è
più conveniente.

Per avere una sola iterazione dovrei avere M = A, ma in questo caso dovrei calcolare l'inversa di A.

Se M è **simile** ad A, allora la matrice G è _buona_. Ovvero intendiamo che $M^{-1}A \sim I$, ovvero ho degli 1 sulla
diagonale e degli elementi molto vicini a 0. Garantisce la convergenza e una buona velocità di convergenza.

Questa esigenza va coniugata con la **struttura semplice** della matrice A. M ha una "struttura semplice", ovvero
è diagonale, triangolare superiore o triangolare inferiore. In questo modo posso sfruttare procedimenti a basso costo
computazionale. Garantisce che stiamo eseguendo operazioni nell'ordine di $n^2$ per risolvere il sistema.

Nel concreto calcoliamo $Mx = Mx - Ax + b$, cioè $Mx^{(k+1)} = (M-A)x^{(k)} + b = \Sigma^{(k)} \in \mathbb{R}^n$.

$Mx^{(k+1)} = \Sigma^{(k)}$.

Se risolvessimo con $n^3$ non sarebbe più conveniente usare un metodo iterativo.

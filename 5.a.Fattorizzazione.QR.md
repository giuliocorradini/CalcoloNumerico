# Fattorizzazione QR

A è una matrice quadrata di dimensioni n. Sotto l'ipotesi che sia nonsingolare, allora esiste una matrice
ortogonale Q e una matrice triangolare superiore nonsingolare R tale che:

$$A = QR$$

Se Q è ortogonale allora $Q^T = Q^{-1}$, inoltre $Q\ Q^T = Q^T\ Q = I$

Posso dimostrare questo teorema con vari metodi: con trasformazioni elementari di Householder, di Givens,
con il procedimento Gram-Schmidt oppure per pavimentazione.

## Trasformazioni di Householder

Dato un vettore colonna $v \in \mathbb{R}^n$ con $v \ne 0$, definiamo la trasformazione elementare di Householder
associata a v la matrice:

$$U = I - \frac{1}{\alpha}vv^T$$

dove

$$\alpha = \frac{1}{2}||v||^2_2$$

(ricordiamo che la norma 2 al quadrato è $v_1^2 + v_2^2 + \dots + v_n^2$).

### Matrice U

$vv^T$ è una matrice siffatta

$$
\begin{pmatrix} v_{1}^2 & v_1v_2 & \dots & v_1v_n \\ v_1v_2 & v_2^2 & \dots & v_2v_n \\ \vdots & \vdots & \ddots & \vdots \\ v_1v_n & v_2v_n & \dots & v_n^2 \end{pmatrix}
$$

### Simmetria

Per proprietà commutativa della moltiplicazione su numeri reali, la matrice è simmetrica $(vv^T)^T = vv^T$.

Dimostrazione: $U^T = (I - \frac{1}{\alpha}vv^T)^T = I^T - \frac{1}{\alpha}(vv^T)^T = U$.

### Ortogonalità

Devo dimostrare $U^TU = UU^T$.

$$U^TU = UU$$

$$= (I - \frac{1}{\alpha}vv^T)(I - \frac{1}{\alpha}vv^T)$$

$$= I - I \frac{1}{\alpha}vv^T - \frac{1}{\alpha}vv^T I + \frac{1}{\alpha^2} v\ v^Tv\ v^T = \dots$$

ma $v^tv = ||v||^2 = 2\alpha$, quindi $\dots = I - 2 \frac{1}{\alpha}vv^T + \frac{2}{\alpha} vv^T = I$.

## Osservazione

Voglio calcolare $z = Uy$, dove U è la trasformazione elementare di Householder.

La prima opzione è calcolare U come $I - \frac{1}{\alpha} vv^T$ e poi
calcolo z = Uy.

La seconda opzione è fare un po' di algebra, facendo diventare
$z = y - \frac{1}{\alpha}vv^Ty$.
Prima eseguo $v^Ty$, che è un prodotto riga per colonna, che mi ritorna uno scalare.

L'alternativa 2 è computazionalmente più conveniente:

- con la prima devo calcolare $n^2$ prodotti, poi $\frac{n^2}{2}$ prodotti (con uno scalare) e infine $n^2$ somme.
$\Omega(n^2)$.

- con la seconda ho $n$ prodotti, poi ho $n$ differenze. $\Omega(n)$.

## Proprietà sigma

### Idea

Voglio eliminare gli elementi del triangolo inferiore di A usando le trasformazioni elementari di Householder invece
di quelle di Gauss.

Scegliendo opportunamente U, possiamo trovare un valore $\sigma$ tale che $Uz = - \sigma e_1$.

### Costruzione

Prendiamo uno $z \in \mathbb{R}^n \setminus \{0\}$. Scelgo $v = z + \sigma e_1$, dove $e_1$ è la prima colonna della
matrice identità di ordine n.

$$\sigma = ||z||_2 = \sqrt{z_1^2 + z_2^2 + \dots + z_n^2}$$

Che ci rappresenta la distanza in $\mathbb{R}^n$ del vettore z dall'origine.

### Dimostrazione

Calcoliamo $\alpha$

$$
\alpha = \frac{1}{2}||v||^2
= \frac{1}{2}v^Tv
= \frac{1}{2}(z + \sigma e_1)^T (z + \sigma e_1)
= \frac{1}{2}(z^T + \sigma e_1^T) (z + \sigma e_1)
$$

con la proprietà distributiva sviluppo il prodotto

$$
\frac{1}{2} (z^Tz + \sigma z e_1 + \sigma e_1^T z + \sigma^2 e_1^T e_1)

= \frac{1}{2} (||z||^2 + \sigma z_1 + \sigma z_1 + \sigma^2 ||e_1||^2)

= \frac{1}{2} (||z||^2 + 2 \sigma z_1 + \sigma^2)

= \frac{1}{2} (\sigma^2 + 2 \sigma z_1 + \sigma^2)

= \frac{1}{2} 2 (\sigma^2 + \sigma z_1)

= \sigma^2 + \sigma z_1
$$

Calcoliamo Uz:

$$
= (I - \frac{1}{\alpha} vv^T) z
= [I - \frac{1}{\alpha} (z + \sigma e_1) (z + \sigma e_1)^T] z
= Iz - \frac{1}{\alpha} (z + \sigma e_1) (z + \sigma e_1)^T z
= Iz - \frac{1}{\alpha} (z + \sigma e_1) (z^T + \sigma e_1^T) z
= Iz - \frac{1}{\alpha} (z + \sigma e_1) (z^Tz + \sigma e_1^T z)
= Iz - \frac{1}{\alpha} (z + \sigma e_1) (\sigma^2 + \sigma z_1)
$$

a questo punto sostituisco $(\sigma^2 + \sigma z_1)$ con $\alpha$

$$
= Iz - (z + \sigma e_1) = z - z - \sigma e_1 = - \sigma e_1
$$

che ha queste fattezze

$$
- \sigma e_1 = \begin{pmatrix} -\sigma \\ 0 \\ \vdots \\ 0 \end{pmatrix}
$$

C'è un caso particolare in cui non abbiamo bisogno di fare la trasformazione,
ovvero quando la colonna contiene un singolo valore negativo.
Questa genererà un vettore v nullo, sui cui non posso applicare la trasformazione
elementare di Householder. Ma non ne ho neanche bisogno dato che il vettore è già "pronto".

## La fattorizzazione

Devo arrivare a $A = QR$

Al primo passaggio calcolo la trasfomazione di Householder $U_1$ su $v_1 = a^{(1)} + \sigma_1 e_1$.

La mia matrice A è:

$$
A=\begin{pmatrix} a_{11} & a_{12} & \dots & a_{1n} \\ a_{21} & a_{22} & \dots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{n1} & a_{n2} & \dots & a_{nn} \end{pmatrix}
$$

Posso vedere la mia matrice come una riga di colonne.

### Passo 1

Per costruire la prima trasformazione scelgo $v_1 = A^{(1)} + \sigma_1 e_1$, ricordandoci che $\sigma_i = ||A^{(i)}||$

$$
U_1 = I - \frac{1}{\alpha_1} v_1 v_1^T
$$

$U_1A = (U_1a^{(1)}, U_2a^{(2)}, \dots, U_n,a^{(n)})$.

La $U_1$ è la trasformazione elementare di Householder, e quando la moltiplico per la prima colonna,
per come l'ho costruita sarà z, ottengo proprio una colonna con $-\sigma_1$ e la sottocolonna annullata.

### Passo 2

Siccome voglio annullare la sottocolonna $a_{22}$ fino a $a_{n2}$, lasciando invariato $a_{12}$ allora
applico la mia trasformazione a $\tilde{A}_2$ sottomatrice che va da $A_{22}$ fino a $A_{nn}$. La nuova
dimensione diventa (n-1)(n-1).

Il nuovo vettore $v_2$ per costruire la nuova trasformazione di Householder, sarà:

$$
v_2 = \tilde{A}^{(1)} %colonna 1 di A tilde
+ \sigma_2 e_1
$$

quindi è costruita come prima, solo su una sottomatrice.

La trasformazione elementare di Householder $\tilde{U}_2$ associata al vettore $v_2$, di dimensioni (n-1)(n-1).
La matrice $U_2$ vera e propria è un'accrescimento di questa matrice.

$$
\tilde{U}_2 \tilde{A}_2 = (\tilde{U}_2 \tilde{A}_2^{(1)}, \tilde{U}_2 \tilde{A}_2^{(2)}, \dots, \tilde{U}_2 \tilde{A}_2^{(n-1)})
$$

Come faccio ad "avvitare" questa nuova matrice all'interno della precedente?.

Devo moltiplicare la mia $A_2$ per $U_2$ che si costruisce mettendo $e_1$ della matrice identità (1 solo in posizione (1,1))
e poi metto la sottomatrice $\tilde{U}_2$.

E poi si procede iterativamente così: diminuisco di uno la dimensione, e mi costruisco la trasformazione di Householder...

Alla fine dei passaggi: $R = A_n = U_{n-1}A_{n-1} = U_{n-1}U_{n-2} \dots U_1 A$

Siccome le U sono trasformazioni elementari di Householder (simmetriche e orotogonali), allora $Q^{-1}
= U_{n-1}U_{n-2} \dots U_1 = Q^T = Q$.

Sotto l'ipotesi che $det(A) \ne 0$, $A=QR$.

$$Ax = b$$

$$
QRx = b
$$

$$
Qy = b
$$

Ottengo un sistema con una matrice **ortogonale**, quindi per trovare y faccio
il prodotto matrice vettore $Q^t b$.

$$
Rx = y
$$

Ottengo un sistema triangolare superiore (sostituzione all'indietro).

## Altri usi

La fattorizzazione QR è utile per sistemi dove $m<=n$ (sistemi rettangolari sovradimensionati).
In questi casi ho più equazioni che incognite, quindi almeno una riga deve essere linearmente
dipendente.

Per fattorizzare devo passare alla dimensione $n \times n$. $A=Q \begin{pmatrix}R \\ O\end{pmatrix}$.

## Stabilità

Per le fattorizzazioni si definisce la stabilità come un upper bound per gli elementi dei fattori.
Quando si ha stabilità forte, i limiti non dipendono dalla dimensione della matrice, altrimenti si
parla di stabilità debole.

Gauss con pivoting parziale e QR hanno stabilità debole, mentre Cholesky gode di stabilità forte.
Ma in generale QR è più stabile di Gauss.
